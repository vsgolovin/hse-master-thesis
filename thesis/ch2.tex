\subsection{Данные для обучения и извлечение эмбеддингов}\label{ssec:data}

Здесь мы практически полностью повторяем описанный в~\cite{isrpaper} подход.
Для обучения и тестирования моделей мы использовали фреймворк
PyTorch~\cite{pytorch}, исходный код доступен на платформе GitHub\footnote{
    \href{https://github.com/vsgolovin/interactive-speaker-recognition}
    {https://github.com/vsgolovin/interactive-speaker-recognition}
}. Единственным (но очень существенным) отличием является использованная
размерность эмбеддингов. Перед тем как перейти к обсуждению этого момента,
расскажем про исходные данные.

Итак, для обучения и тестирования моделей мы использовали датасет
TIMIT~\cite{timit}. Он составлен из аудиозаписей речи 630~дикторов, говорящих на
8~основных диалектах американского английского языка. Эти дикторы поделены на
обучающую (\textit{train}) и тестовую (\textit{test}) выборки, в первую входят
468~дикторов, во вторую --- 162. Для обучения нейросетевых моделей мы также
создавали валидационную выборку, в которую выделялись 20\% дикторов из
обучающей.

Каждый из дикторов произносит 10~фонетически насыщенных предложений. При этом 2
из 10 предложений являются общими для всех дикторов\footnote{
    Эти предложения:
    \begin{itemize}
        \item \textit{She had your dark suit in greasy wash water all year.}
        \item \textit{Don't ask me to carry an oily rag like that.}
    \end{itemize}
}, остальные 8 уникальны для каждого диктора. Такое разделение позволяет без
особых затруднений подготовить данные, необходимые для описанной
в~\ref{ssec:isr} игры:
\begin{itemize}
    \item 2~общих предложения можно использовать для получения аудиозаписей
    слов.  Для этого разделим аудиозаписи этих предложений по временным
    отметкам, предоставленным создателями датасета. В результате получим 20
    аудиозаписей слов\footnote{Аналогично~\cite{isrpaper} мы не используем слово
    \textit{an}.} для каждого диктора.
    \item 8~уникальных для каждого диктора предложений можно использовать для
    получения голосовых подписей --- эмбеддингов дикторов --- просто при помощи
    усреднения эмбеддингов аудиозаписей этих предложений.
\end{itemize}

В качестве векторов признаков использовались эмбеддинги
\xvector{}~\cite{xvectorspaper}. Весь процесс преобразования аудиозаписей в
векторы признаков осуществлялся с помощью библиотеки Kaldi~\cite{kaldi}. На
первом этапе рассчитывалиcь мел-частотные кепстральные коэффициенты\footnote{
    Параметры аналогичны использованным в~\cite{isrpaper} и определяются
    требованиями предобученной модели.
} и производилось детектирование голосовой
активности (Voice Activity Detection). Полученные векторы признаков поступали на
вход предобученной нейронной сети~\cite{sre16model}. В качестве эмбеддингов
использовались данные со второго 512-мерного слоя.

Здесь, как уже было сказано ранее, мы отступаем от оригинальной
работы~\cite{isrpaper}, где использовались 128-мерные эмбеддинги. На это есть
две причины. Во-первых, из приведенных в~\cite{isrpaper} комментариев
неочевидно\footnote{
    Цитата: \textit{We then process the MFCCs features through a pretrained
    X-Vector network to obtain a high quality voice embedding of fixed dimension
    128, where the X-Vector network is trained on augmented Switchboard, Mixer~6
    and NIST SREs}.
}, как производилось понижение размерности.
Во-вторых, мотивация такого преобразования тоже неочевидна. Уже первые проведенные
нами эксперименты показали, что при использовании 512-мерных эмбеддингов точность
идентификации оказывается существенно выше приведенных в~\cite{isrpaper} значений.

\subsection{Обучение \guesser{}}

Первой обучается нейронная сеть \guesser{}, выполняющая выбор из $K$ дикторов
при помощи $T$ аудиозаписей произнесенных слов. Как уже было сказано ранее, эта
нейросеть тренируется в режиме обучения с учителем, дикторы и произносимые слова
выбираются случайно, в качестве функции потерь используется кросс-энтропия.
Процесс вычисления значения функции потерь для одной игры можно записать
следующим образом:
\input{guesser_forward.tex}

Из-за того, что мы увеличили размерность эмбеддингов в 4 раза по сравнению 
с~\citeisr{} пропорционально увеличились и размерности слоёв \guesser{}. Из-за
этого нам пришлось изменить гиперпараметры, в частности мы сильно уменьшили темп
обучения (\textit{learning rate}).

\begin{figure}[!h]
    \centering
    \includegraphics[scale=\pltscale]{../plots/word_sweep.pdf}\\
    \includegraphics[scale=\pltscale]{../plots/guest_sweep.pdf}
    \caption{Зависимость точности \guesser{} обученного нами и авторами
    \citeisr{} от (а) числа запрошенных слов $T$, (б) числа дикторов $K$.
    Модели обучены в режиме $K = 5$, $T = 3$.}
    \label{fig:guesser_test}
\end{figure}

Как и в оригинальной статье, для сравнения моделей мы строим графики
\textit{word} и \textit{guest sweep}. Т.\,е. мы обучаeм модель в режиме с $K =
5$ дикторами и $T = 3$ запрашиваемыми словами, а затем тестируем её в режимах с
отличным числом дикторов или слов. Здесь и далее, если это не оговорено
отдельно, для расчёта точности проводятся $20000$ игр среди дикторов из тестовой
выборки, эксперименты повторяются по 5~раз c различным \texttt{seed} генератора
случайных чисел.

По приведенным на рис.~\ref{fig:guesser_test} результатам видно, что увеличение размерности
эмбеддингов существенно улучшает точность идентификации, разница особо
велика в режимах с большим числом дикторов $K$.

\subsection{Обучение \enquirer{}}

Для обучения \enquirer{} --- модели для выбора слов --- уже нужна обученная
модель \guesser{}. На этом этапе используется обучение с подкреплением,
псевдокод для 1~игры приведён ниже.

Как видно из приведенного псевдокода, награда выдается в том случае, когда
\guesser{} правильно угадывает диктора. Для обучения мы использовали алгоритм
PPO~\cite{schulman2017proximal} --- здесь мы снова повторяем подход
авторов~\citeisr{}. В целом выбор метода выглядит разумным --- PPO
зарекомендовал себя как простой и универсальный алгоритм, позволяющий достигать
хороших результатов. Однако некоторые особенности нашей задачи --- дискретное
пространство действий, малая длительность эпизодов --- выглядят лучше подходящими
для off-policy алгоритмов. К сожалению, у нас не нашлось времени, чтобы проверить
эту гипотезу.

\newpage
\input{enquirer_forward.tex}

\begin{figure}[H]
    \centering
    \includegraphics[scale=\pltscale]{../plots/word_sweep_enq.pdf}
    \includegraphics[scale=\pltscale]{../plots/guest_sweep_enq.pdf}
    \caption{Зависимость точности SR-систем с различными методами выбора слов
    --- нейросетевым (\enquirer{}) и случайным (random agent) --- от
    (а) числа запрашиваемых слов $T$, (б) числа дикторов $K$. Модели обучены в
    режиме $K = 5$, $T = 3$.}
    \label{fig:enquirer_test}
\end{figure}

Приведенные на рис.~\ref{fig:enquirer_test} результаты свидетельствуют о том, что
нейросеть действительно успешно обучается --- точность оказываются заметно выше,
чем с случае случайного выбора слов. Как и в случае повышения размерности
эмбеддингов, особенно большое различие наблюдается в режимах с большим числом
дикторов.

\subsection{Эвристическая модель выбора слов}

Очевидно, что агент, выбирающий запрашиваемые слова случайным образом, не
является тяжелым противником для нейросетевого агента. Для более трезвой
оценки возможностей последнего, логично сравнивать его с каким-то более сложным
алгоритмом.

Здесь мы снова немного отходим от оригинальной статьи. И опять основной
причиной является тот факт, что в \citeisr{} отсутствует точное описание
использованного в качестве бейзлайна эвристического алгоритма выбора слов.
Из приведенного в работе объяснения\footnote{
    Цитата: \textit{We curated a list of the most discriminant
    words (words that increase globally the recognition scores)
    and sample among those instead of the whole list.}
}
общий подход понятен --- сэмплирование производится не из всех $20$ слов, а
из тех, которые в среднем показывают самую высокую точность. При этом остаются
непонятными следующие детали:
\begin{enumerate}
    \item Из скольки слов производится сэмплирование, и меняется ли это число
    в зависимости от числа запрашиваемых слов?
    \item Производится ли сэмплирование равномерно, или вероятность выбрать
    слово пропорциональна достигаемой при выборе этого слова средней точности?
\end{enumerate}

Именно такие вопросы возникли у нас при создании эвристического агента. Первым
же этапом стала оценка слов --- расчёт средней точности, которая достигается
случайным агентом в тех играх, когда он выбрал то или иное слово. Для этого мы
протестировали \guesser{} в $100000$ эпизодов с $K = 5$, $T = 3$, а также
случайным выбором слов без повторений (рис.~\ref{fig:wscores}). Мы рассчитывали точность для каждого слова,
учитывая только те эпизоды, в которых это слово было выбрано. Фактически мы оценивали
условную вероятность связки \guesser{}--случайный агент правильно выбрать диктора
при условии, что одно слово уже было выбрано.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=\pltscale]{../plots/word_scores.pdf}
    \caption{Средняя точность \guesser{} на валидационной выборке в тех
    эпизодах, когда соответствующее слово было выбрано (остальные выбирались
    случайно).}
    \label{fig:wscores}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=\pltscale]{../plots/word_sweep_heuristic.pdf}
    \includegraphics[scale=\pltscale]{../plots/guest_sweep_heuristic.pdf}
    \caption{Зависимость точности SR-систем с различными методами выбора слов
    --- нейросетевым и эвристическим агентами --- от (а) числа запрашиваемых
    слов $T$, (б) числа дикторов $K$. Модели обучены в режиме $K = 5$, $T = 3$.}
\end{figure}

После этого мы стали тестировать различные модификации эвристического агента,
отличавшиеся числом использованных слов и методом сэмплирования. Эксперименты
показали, что наилучшие результаты достигаются при использовании
``детерминированного'' агента, всегда выбирающего одни и те же слова с
наибольшей средней точностью.  В таком случае говорить о каком-либо
сэмплировании неуместно, поэтому такой агент, по всей видимости, отличается от
использованного в оригинальной статье.

В данном случае преимущество \enquirer{} проявляется только в режимах с большим
числом дикторов. В стандартном режиме с $K = 5$ дикторами и $T = 3$, в отличие
от~\citeisr{}, мы не наблюдаем сколько-нибудь существенной разницы между двумя
агентами.

В таком случае возникает резонный вопрос --- не сходится ли \enquirer{} к такой
же политике, что и эвристический агент? Ответ на этот вопрос --- отрицательный,
протестированный \enquirer{} в основном выбирает из 5 слов (ещё 2 используются
редко), в то время как эвристический агент всегда использует 3 тех же слова. Из
этого можно предположить, что \enquirer{} обучен недостаточно хорошо, возможно,
другие гиперпараметры или алгоритм обучения позволили бы улучшить результаты.

\subsection{Обучение в других режимах}

Другой логичный вопрос, возникающий при обсуждении графиков \textit{word} и
\text{guest sweep} --- является ли стандартный режим ($K = 5$ дикторов и
$T = 3$ запрашиваемых слова) оптимальным для обучения моделей? Не будут ли
результаты лучше, если мы будем обучать и тестировать модели в одном и том же
режиме? Мы также провели ряд экспериментов и пришли к следующим выводам:
\begin{itemize}
    \item Общее правило --- более тяжелые режимы позволяют улучшить точность.
    В первую очередь это касается увеличения числа дикторов, ситуация с
    уменьшением числа слов менее однозначная. Пример этого эффекта
    демонстрирует табл.~\ref{tab:isr_training}.
    \item Основные улучшения наблюдаются в работе \guesser{}, в то же время
    \enquirer{} оказывается нечувствительным к режиму обучения.
    \item Обучение при $T = 1$ является специфической задачей. Во-первых,
    модель, обученная в таком режиме, показывает (относительно) хорошие
    результаты только в нём. Во-вторых, \enquirer{} в целом плохо справляется
    с этой задачей, часто уступая максимально простой политике, всегда
    выбирающей одно и то же слово.
\end{itemize}

\begin{table}[htb]
    \centering
    \begin{tabular}{c c c}
        \toprule
        Выбор слов & Режим обучения & Точность\\
        \midrule
        случайный & \multirow{3}{4em}{$K = 5$\newline$T = 3$} & 0.937 \\
        \enquirer{} & & 0.982\\
        эвристика & & 0.984\\
        \midrule
        случайный & \multirow{3}{4em}{$K = 20$\\$T = 2$} & 0.951 \\
        \enquirer{} & & 0.989\\
        эвристика & & 0.988\\
        \bottomrule
    \end{tabular}
    \caption{Точность идентификации, $K = 5$ дикторов, $T = 3$ запрашиваемых
             слова.}
    \label{tab:isr_training}
\end{table}

\subsection{Выводы и результаты по главе}

\begin{itemize}
    \item Предложенный в~\citeisr{} действительно работает, нейросетевая модель
    выбора слов действительно позволяет увеличить точность распознавания.
    \item В оригинальной статье было выполнено понижение размерности
    эмбеддингов. Как именно и зачем это было сделано --- неизвестно. Мы обучили
    модель на ``полноразмерных'' 512-мерных эмбеддингов и наши результаты
    оказались существенно лучше, чем в оригинальной статье.
    \item Хотя предложенный подход действительно работает, его преимущество над
    простым эвристическим подходом в среднем оказывается небольшим. Возможно,
    это можно исправить с помощью оптимизации процедуры обучения.
\end{itemize}